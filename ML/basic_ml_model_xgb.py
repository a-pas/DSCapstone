# -*- coding: utf-8 -*-
"""Basic_ML_Model_XGB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17JRx7T2YicBBoxhmLPTiB3MjfxD6K1hq
"""

import pandas as pd

df1 = pd.read_csv("combined_data.csv")

df1.info()

# ANOVA - It assess how a numerical feature impacts a categorical one

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Assuming df is your DataFrame and you are testing 'OFFENSE' against numerical variables

# List of numerical columns
numeric_cols = ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'WARD', 'DISTRICT', 'PSA', 'LATITUDE', 'LONGITUDE']

# Perform ANOVA for each numerical column with the categorical column 'OFFENSE'
for col in numeric_cols:
    formula = f"{col} ~ C(OFFENSE)"  # ANOVA formula
    model = ols(formula, data=df1).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)

    # Output the result
    print(f"ANOVA results for {col} and OFFENSE:")
    print(anova_table)
    print("\n")

# Chi-Square Test Between Categorical Variables

from scipy.stats import chi2_contingency

# Assuming df is your DataFrame and you want to test categorical columns with 'OFFENSE'

# List of categorical columns to test against 'OFFENSE'
categorical_cols = ['SHIFT', 'METHOD', 'BLOCK', 'ANC', 'VOTING_PRECINCT']

# Perform Chi-Square test for each categorical column with the target categorical column 'OFFENSE'
for col in categorical_cols:
    # Create a contingency table
    contingency_table = pd.crosstab(df1[col], df1['OFFENSE'])

    # Perform Chi-Square test
    chi2, p, dof, expected = chi2_contingency(contingency_table)

    # Output the result
    print(f"Chi-Square Test for {col} and OFFENSE:")
    print(f"Chi2 Statistic = {chi2}, p-value = {p}")
    print(f"Degrees of Freedom = {dof}\n")

from sklearn.model_selection import train_test_split

# Define X (features) and y (target)
x_independent = df1.drop(columns=['OFFENSE', 'DAY'])  # Drop the target variable and DAY
y_dependent = df1['OFFENSE']  # Target variable

# Define categorical and numerical columns
categorical_cols = ['SHIFT', 'METHOD', 'BLOCK', 'ANC', 'VOTING_PRECINCT']  # Replace with your actual categorical columns
numerical_cols = ['YEAR', 'MONTH', 'HOUR', 'MINUTE', 'WARD', 'DISTRICT', 'PSA', 'LATITUDE', 'LONGITUDE']  # Add other numerical columns as needed

# Define categorical and numerical columns
categorical_cols = ['SHIFT', 'METHOD', 'BLOCK', 'ANC', 'VOTING_PRECINCT']  # Replace with your actual categorical columns
numerical_cols = ['YEAR', 'MONTH', 'HOUR', 'MINUTE', 'WARD', 'DISTRICT', 'PSA', 'LATITUDE', 'LONGITUDE']  # Add other numerical columns as needed

# One-Hot Encode Categorical Variables using pd.get_dummies()
# drop_first=True avoids dummy variable trap
X_encoded = pd.get_dummies(x_independent, columns=categorical_cols, drop_first=True)

y_dependent1 = pd.get_dummies(y_dependent, columns="OFFENSE", drop_first=True)

# Combine the encoded categorical columns with numerical columns (already combined in the previous step)
# Split the data: 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_dependent1, test_size=0.2, random_state=42)

from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# Create and configure the XGBoost model
model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Fit the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))